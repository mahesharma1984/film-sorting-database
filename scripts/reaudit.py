#!/usr/bin/env python3
"""
scripts/reaudit.py — Library re-classification audit (Issue #31)

Reads output/library_audit.csv (generated by audit.py), re-runs each organized film
through the current classification pipeline, compares the result to its current folder
location, and writes a discrepancy report.

NEVER moves files. NEVER modifies caches (Stage 1). Read-only diagnostic tool.

Usage:
    python scripts/reaudit.py                    # Stage 1: cache-only ($0 cost)
    python scripts/reaudit.py --enrich           # Stage 2: live API for no_data films
    python scripts/reaudit.py --review           # Stage 3: markdown report
    python scripts/reaudit.py --audit PATH       # custom audit CSV input
"""

import sys
import csv
import json
import argparse
import logging
from collections import defaultdict
from pathlib import Path
from typing import Optional, Dict, List

# Project root on sys.path so we can import from classify.py and lib/
sys.path.insert(0, str(Path(__file__).parent.parent))

from classify import FilmClassifier, ClassificationResult, load_config, get_decade
from lib.parser import FilenameParser

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Tiers to audit — skip Staging, Unsorted, Out (only organized tiers matter)
AUDIT_TIERS = {'Core', 'Reference', 'Satellite', 'Popcorn'}

# Reason codes that map to a confidence label
_HIGH_REASONS = {'core_director', 'explicit_lookup', 'reference_canon'}
_MEDIUM_REASONS = {'country_satellite', 'tmdb_satellite', 'user_tag_recovery'}


# ---------------------------------------------------------------------------
# Cache-only API client
# ---------------------------------------------------------------------------

class CacheOnlyClient:
    """
    Reads an existing JSON cache (tmdb_cache.json or omdb_cache.json) and
    returns cached results. Returns None on cache miss — no network calls.

    Implements the same .search_film() interface as TMDbClient / OMDbClient
    so it can be swapped into FilmClassifier after construction.
    """

    def __init__(self, cache_path: Path):
        self.cache_path = cache_path
        self.cache: Dict = {}
        if cache_path.exists():
            try:
                with open(cache_path, 'r', encoding='utf-8') as f:
                    self.cache = json.load(f)
                logger.info("Loaded cache %s (%d entries)", cache_path.name, len(self.cache))
            except Exception as e:
                logger.warning("Could not load cache %s: %s", cache_path, e)
        else:
            logger.info("Cache not found: %s (all lookups will return None)", cache_path)

    def _make_cache_key(self, title: str, year: Optional[int]) -> str:
        return f"{title}|{year if year else 'None'}"

    def search_film(self, title: str, year: Optional[int] = None) -> Optional[Dict]:
        """Cache lookup only. Returns None on miss."""
        key = self._make_cache_key(title, year)
        return self.cache.get(key)


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------

def _make_classifier(config_path: Path, live_api: bool = False) -> FilmClassifier:
    """
    Build a FilmClassifier.

    live_api=False (default): cache-only mode — FilmClassifier built with
        no_tmdb=True then cache clients swapped in.
    live_api=True: normal mode with real API clients (for --enrich).
    """
    classifier = FilmClassifier(config_path, no_tmdb=not live_api)

    if not live_api:
        # Swap in cache-only clients so classify() reads caches without network calls
        classifier.tmdb = CacheOnlyClient(Path('output/tmdb_cache.json'))
        classifier.omdb = CacheOnlyClient(Path('output/omdb_cache.json'))

    return classifier


def _confidence_label(reason: str, tmdb_id: Optional[int]) -> str:
    """Map ClassificationResult.reason → human confidence label."""
    if reason in _HIGH_REASONS:
        return 'high'
    if reason in _MEDIUM_REASONS:
        return 'medium'
    if reason.startswith('unsorted') or tmdb_id is None:
        return 'none'
    return 'low'


def _discrepancy_type(result: ClassificationResult, current_tier: str,
                      current_category: str) -> str:
    """Determine discrepancy_type string. Returns '' for a match."""
    classified_tier = result.tier
    classified_category = result.subdirectory or ''

    # Matching check
    if classified_tier == current_tier and classified_category == current_category:
        return ''

    # Unsorted outcomes
    if classified_tier == 'Unsorted':
        # no_data: no API data available AND no director found — cannot reclassify
        if result.tmdb_id is None and not result.director:
            return 'no_data'
        return 'unroutable'

    if classified_tier != current_tier:
        return 'wrong_tier'

    return 'wrong_category'


def _build_notes(result: ClassificationResult, discrepancy: str) -> str:
    """Build human-readable notes string."""
    reason = result.reason

    if discrepancy == '':
        return f'Confirmed: {reason}'

    if discrepancy == 'no_data':
        return 'No API data; manual review required'

    if discrepancy == 'unroutable':
        return f'Classifier returns Unsorted ({reason}); investigate routing rules'

    # Build a short description of where the classifier wants to put it
    dest = result.destination.rstrip('/')
    if discrepancy == 'wrong_tier':
        return f'Classifier routes to {dest} ({reason})'

    if discrepancy == 'wrong_category':
        return f'Classifier routes to {dest} ({reason})'

    return reason


def _load_audit_csv(audit_path: Path) -> List[Dict]:
    """Read library_audit.csv and return rows for organized tiers only."""
    if not audit_path.exists():
        logger.error("Audit file not found: %s — run audit.py first", audit_path)
        sys.exit(1)

    rows = []
    with open(audit_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            if row.get('tier') in AUDIT_TIERS:
                rows.append(row)

    logger.info("Loaded %d organized-tier films from %s", len(rows), audit_path)
    return rows


# ---------------------------------------------------------------------------
# Core audit pass
# ---------------------------------------------------------------------------

def run_audit_pass(audit_rows: List[Dict], classifier: FilmClassifier) -> List[Dict]:
    """
    Classify each film in audit_rows and build report rows.
    Returns list of report dicts (one per film).
    """
    parser = FilenameParser()
    report_rows = []

    for i, row in enumerate(audit_rows, 1):
        filename = row.get('filename', '')
        current_tier = row.get('tier', '')
        current_category = row.get('subdirectory', '')
        current_decade = row.get('decade', '')

        try:
            metadata = parser.parse(filename)
            result = classifier.classify(metadata)
        except Exception as e:
            logger.error("Error classifying '%s': %s", filename, e)
            result = None

        if result is None:
            report_rows.append({
                'filename': filename,
                'current_tier': current_tier,
                'current_category': current_category,
                'current_decade': current_decade,
                'classified_tier': 'Error',
                'classified_category': '',
                'classified_decade': '',
                'match': 'false',
                'discrepancy_type': 'error',
                'confidence': 'none',
                'notes': 'Exception during classification',
            })
            continue

        classified_tier = result.tier
        classified_category = result.subdirectory or ''
        classified_decade = result.decade or ''

        discrepancy = _discrepancy_type(result, current_tier, current_category)
        match = 'true' if discrepancy == '' else 'false'
        confidence = _confidence_label(result.reason, result.tmdb_id)
        notes = _build_notes(result, discrepancy)

        report_rows.append({
            'filename': filename,
            'current_tier': current_tier,
            'current_category': current_category,
            'current_decade': current_decade,
            'classified_tier': classified_tier,
            'classified_category': classified_category,
            'classified_decade': classified_decade,
            'match': match,
            'discrepancy_type': discrepancy,
            'confidence': confidence,
            'notes': notes,
        })

        if i % 100 == 0:
            logger.info("Processed %d/%d...", i, len(audit_rows))

    return report_rows


# ---------------------------------------------------------------------------
# Stage 2: --enrich
# ---------------------------------------------------------------------------

def enrich_no_data(report_rows: List[Dict], config_path: Path) -> int:
    """
    Re-classify no_data rows using live API queries. Updates rows in-place.
    Returns count of rows that resolved to a non-Unsorted tier.
    """
    no_data_rows = [r for r in report_rows if r['discrepancy_type'] == 'no_data']
    if not no_data_rows:
        logger.info("No no_data rows to enrich.")
        return 0

    logger.info("Enriching %d no_data films via live API...", len(no_data_rows))
    classifier = _make_classifier(config_path, live_api=True)
    parser = FilenameParser()
    resolved = 0

    for row in no_data_rows:
        filename = row['filename']
        try:
            metadata = parser.parse(filename)
            result = classifier.classify(metadata)
        except Exception as e:
            logger.error("Error enriching '%s': %s", filename, e)
            continue

        current_tier = row['current_tier']
        current_category = row['current_category']
        discrepancy = _discrepancy_type(result, current_tier, current_category)

        row['classified_tier'] = result.tier
        row['classified_category'] = result.subdirectory or ''
        row['classified_decade'] = result.decade or ''
        row['match'] = 'true' if discrepancy == '' else 'false'
        row['discrepancy_type'] = discrepancy
        row['confidence'] = _confidence_label(result.reason, result.tmdb_id)
        row['notes'] = _build_notes(result, discrepancy)

        if result.tier != 'Unsorted':
            resolved += 1

    logger.info("Enrichment: %d/%d no_data rows resolved to a tier", resolved, len(no_data_rows))
    return resolved


# ---------------------------------------------------------------------------
# Stage 3: --review
# ---------------------------------------------------------------------------

_ACTION_MAP = {
    'wrong_tier': '[RECLASSIFY]',
    'wrong_category': '[RECLASSIFY]',
    'no_data': '[MANUAL REVIEW]',
    'unroutable': '[INVESTIGATE ROUTING]',
    '': '[CONFIRMED]',
}

_CONFIDENCE_ORDER = {'high': 0, 'medium': 1, 'low': 2, 'none': 3}


def write_review_report(report_rows: List[Dict], output_path: Path):
    """Write grouped markdown review report."""
    # Group by current_category (or current_tier for tiers without categories)
    by_category: Dict[str, List[Dict]] = defaultdict(list)
    for row in report_rows:
        key = row['current_category'] or row['current_tier']
        by_category[key].append(row)

    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("# Library Re-Audit Review\n\n")
        f.write("Generated by `scripts/reaudit.py --review`\n\n")

        total = len(report_rows)
        matches = sum(1 for r in report_rows if r['match'] == 'true')
        f.write(f"**{total} films audited | {matches} confirmed | {total - matches} discrepancies**\n\n")
        f.write("---\n\n")

        for category in sorted(by_category.keys()):
            rows = sorted(by_category[category],
                          key=lambda r: (_CONFIDENCE_ORDER.get(r['confidence'], 3),
                                         r['filename']))
            discrepancies = [r for r in rows if r['match'] == 'false']
            confirmed = len(rows) - len(discrepancies)

            f.write(f"## {category}\n\n")
            f.write(f"{len(rows)} films — {confirmed} confirmed, {len(discrepancies)} discrepancies\n\n")

            if discrepancies:
                f.write("| Filename | Current | Classified | Type | Confidence | Action |\n")
                f.write("|---|---|---|---|---|---|\n")
                for row in rows:
                    if row['match'] == 'false':
                        current = f"{row['current_tier']}/{row['current_category']}/{row['current_decade']}".rstrip('/')
                        classified = f"{row['classified_tier']}/{row['classified_category']}/{row['classified_decade']}".rstrip('/')
                        action = _ACTION_MAP.get(row['discrepancy_type'], '')
                        f.write(f"| {row['filename']} | {current} | {classified} | "
                                f"{row['discrepancy_type']} | {row['confidence']} | {action} |\n")
                f.write("\n")
            else:
                f.write("All films confirmed.\n\n")

    logger.info("Wrote review report to %s", output_path)


# ---------------------------------------------------------------------------
# Output
# ---------------------------------------------------------------------------

def write_report_csv(report_rows: List[Dict], output_path: Path):
    """Write reaudit_report.csv."""
    output_path.parent.mkdir(parents=True, exist_ok=True)
    fieldnames = [
        'filename', 'current_tier', 'current_category', 'current_decade',
        'classified_tier', 'classified_category', 'classified_decade',
        'match', 'discrepancy_type', 'confidence', 'notes',
    ]
    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)
        writer.writeheader()
        writer.writerows(report_rows)
    logger.info("Wrote reaudit report to %s", output_path)


def print_summary(report_rows: List[Dict]):
    """Print summary statistics to stdout."""
    total = len(report_rows)
    matches = sum(1 for r in report_rows if r['match'] == 'true')
    by_type: Dict[str, int] = defaultdict(int)
    for r in report_rows:
        by_type[r['discrepancy_type'] or 'match'] += 1

    print(f"\n{'='*60}")
    print(f"REAUDIT SUMMARY")
    print(f"{'='*60}")
    print(f"Total organized films audited : {total}")
    print(f"Confirmed (match)             : {matches}")
    print(f"Discrepancies                 : {total - matches}")
    print()
    for dtype in ['wrong_tier', 'wrong_category', 'no_data', 'unroutable', 'error']:
        count = by_type.get(dtype, 0)
        if count:
            print(f"  {dtype:<20} : {count}")
    print(f"{'='*60}\n")


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(
        description='Re-audit organized library against current classification rules (Issue #31)'
    )
    parser.add_argument(
        '--audit',
        default='output/library_audit.csv',
        help='Path to library_audit.csv (default: output/library_audit.csv)'
    )
    parser.add_argument(
        '--enrich',
        action='store_true',
        help='Stage 2: query live APIs for no_data films (costs API calls)'
    )
    parser.add_argument(
        '--review',
        action='store_true',
        help='Stage 3: write human-readable markdown report to output/reaudit_review.md'
    )
    parser.add_argument(
        '--config',
        default='config.yaml',
        help='Path to config YAML (default: config.yaml)'
    )
    args = parser.parse_args()

    config_path = Path(args.config)
    if not config_path.exists():
        # Try config_external.yaml as fallback
        config_path = Path('config_external.yaml')
    if not config_path.exists():
        logger.error("No config file found (tried config.yaml, config_external.yaml)")
        sys.exit(1)

    audit_path = Path(args.audit)

    # Stage 1: cache-only classification pass
    logger.info("Stage 1: cache-only classification pass")
    classifier = _make_classifier(config_path, live_api=False)
    audit_rows = _load_audit_csv(audit_path)
    report_rows = run_audit_pass(audit_rows, classifier)

    # Stage 2: enrich no_data films (optional)
    if args.enrich:
        logger.info("Stage 2: enriching no_data films via live API")
        enrich_no_data(report_rows, config_path)

    # Write CSV report
    write_report_csv(report_rows, Path('output/reaudit_report.csv'))
    print_summary(report_rows)

    # Stage 3: markdown review report (optional)
    if args.review:
        logger.info("Stage 3: writing review report")
        write_review_report(report_rows, Path('output/reaudit_review.md'))


if __name__ == '__main__':
    main()
