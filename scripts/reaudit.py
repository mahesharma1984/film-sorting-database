#!/usr/bin/env python3
"""
scripts/reaudit.py — Library re-classification audit (Issue #31)

Reads output/library_audit.csv (generated by audit.py), re-runs each organized film
through the current classification pipeline, compares the result to its current folder
location, and writes a discrepancy report.

NEVER moves files. NEVER modifies caches (Stage 1). Read-only diagnostic tool.

Usage:
    python scripts/reaudit.py                    # Stage 1: cache-only ($0 cost)
    python scripts/reaudit.py --enrich           # Stage 2: live API for no_data films
    python scripts/reaudit.py --review           # Stage 3: markdown report
    python scripts/reaudit.py --audit PATH       # custom audit CSV input
"""

import sys
import csv
import json
import argparse
import logging
import unicodedata
from collections import defaultdict
from pathlib import Path
from typing import Optional, Dict, List

# Project root on sys.path so we can import from classify.py and lib/
sys.path.insert(0, str(Path(__file__).parent.parent))

from classify import FilmClassifier, ClassificationResult, load_config, get_decade
from lib.parser import FilenameParser
from lib.corpus import CorpusLookup

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Tiers to audit — skip Staging, Unsorted, Out (only organized tiers matter)
AUDIT_TIERS = {'Core', 'Reference', 'Satellite', 'Popcorn'}

# Reason codes that map to a confidence label
_HIGH_REASONS = {'core_director', 'explicit_lookup', 'reference_canon'}
_MEDIUM_REASONS = {'country_satellite', 'tmdb_satellite', 'user_tag_recovery'}


# ---------------------------------------------------------------------------
# Cache-only API client
# ---------------------------------------------------------------------------

class CacheOnlyClient:
    """
    Reads an existing JSON cache (tmdb_cache.json or omdb_cache.json) and
    returns cached results. Returns None on cache miss — no network calls.

    Implements the same .search_film() interface as TMDbClient / OMDbClient
    so it can be swapped into FilmClassifier after construction.
    """

    def __init__(self, cache_path: Path):
        self.cache_path = cache_path
        self.cache: Dict = {}
        if cache_path.exists():
            try:
                with open(cache_path, 'r', encoding='utf-8') as f:
                    self.cache = json.load(f)
                logger.info("Loaded cache %s (%d entries)", cache_path.name, len(self.cache))
            except Exception as e:
                logger.warning("Could not load cache %s: %s", cache_path, e)
        else:
            logger.info("Cache not found: %s (all lookups will return None)", cache_path)

    def _make_cache_key(self, title: str, year: Optional[int]) -> str:
        return f"{title}|{year if year else 'None'}"

    def search_film(self, title: str, year: Optional[int] = None) -> Optional[Dict]:
        """Cache lookup only. Returns None on miss."""
        key = self._make_cache_key(title, year)
        return self.cache.get(key)


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------

def _make_classifier(config_path: Path, live_api: bool = False) -> FilmClassifier:
    """
    Build a FilmClassifier.

    live_api=False (default): cache-only mode — FilmClassifier built with
        no_tmdb=True then cache clients swapped in.
    live_api=True: normal mode with real API clients (for --enrich).
    """
    classifier = FilmClassifier(config_path, no_tmdb=not live_api)

    if not live_api:
        # Swap in cache-only clients so classify() reads caches without network calls
        classifier.tmdb = CacheOnlyClient(Path('output/tmdb_cache.json'))
        classifier.omdb = CacheOnlyClient(Path('output/omdb_cache.json'))

    return classifier


def _confidence_label(reason: str, tmdb_id: Optional[int]) -> str:
    """Map ClassificationResult.reason → human confidence label."""
    if reason in _HIGH_REASONS:
        return 'high'
    if reason in _MEDIUM_REASONS:
        return 'medium'
    if reason.startswith('unsorted') or tmdb_id is None:
        return 'none'
    return 'low'


def _nfc(s: str) -> str:
    """Normalize to NFC so accented chars compare equal regardless of encoding."""
    return unicodedata.normalize('NFC', s)


def _discrepancy_type(result: ClassificationResult, current_tier: str,
                      current_category: str) -> str:
    """Determine discrepancy_type string. Returns '' for a match."""
    classified_tier = result.tier
    classified_category = result.subdirectory or ''

    # Matching check — normalize Unicode so e.g. "Almodóvar" (NFD) == "Almodóvar" (NFC)
    if classified_tier == current_tier and _nfc(classified_category) == _nfc(current_category):
        return ''

    # Unsorted outcomes
    if classified_tier == 'Unsorted':
        # no_data: no API data available AND no director found — cannot reclassify
        if result.tmdb_id is None and not result.director:
            return 'no_data'
        return 'unroutable'

    if classified_tier != current_tier:
        return 'wrong_tier'

    return 'wrong_category'


def _build_notes(result: ClassificationResult, discrepancy: str) -> str:
    """Build human-readable notes string."""
    reason = result.reason

    if discrepancy == '':
        return f'Confirmed: {reason}'

    if discrepancy == 'no_data':
        return 'No API data; manual review required'

    if discrepancy == 'unroutable':
        return f'Classifier returns Unsorted ({reason}); investigate routing rules'

    # Build a short description of where the classifier wants to put it
    dest = result.destination.rstrip('/')
    if discrepancy == 'wrong_tier':
        return f'Classifier routes to {dest} ({reason})'

    if discrepancy == 'wrong_category':
        return f'Classifier routes to {dest} ({reason})'

    return reason


def _load_audit_csv(audit_path: Path) -> List[Dict]:
    """Read library_audit.csv and return rows for organized tiers only."""
    if not audit_path.exists():
        logger.error("Audit file not found: %s — run audit.py first", audit_path)
        sys.exit(1)

    rows = []
    with open(audit_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            if row.get('tier') in AUDIT_TIERS:
                rows.append(row)

    logger.info("Loaded %d organized-tier films from %s", len(rows), audit_path)
    return rows


# ---------------------------------------------------------------------------
# Core audit pass
# ---------------------------------------------------------------------------

def run_audit_pass(audit_rows: List[Dict], classifier: FilmClassifier) -> List[Dict]:
    """
    Classify each film in audit_rows and build report rows.
    Returns list of report dicts (one per film).
    """
    parser = FilenameParser()
    report_rows = []

    for i, row in enumerate(audit_rows, 1):
        filename = row.get('filename', '')
        current_tier = row.get('tier', '')
        current_category = row.get('subdirectory', '')
        current_decade = row.get('decade', '')

        try:
            metadata = parser.parse(filename)
            result = classifier.classify(metadata)
        except Exception as e:
            logger.error("Error classifying '%s': %s", filename, e)
            result = None

        if result is None:
            report_rows.append({
                'filename': filename,
                'current_tier': current_tier,
                'current_category': current_category,
                'current_decade': current_decade,
                'classified_tier': 'Error',
                'classified_category': '',
                'classified_decade': '',
                'match': 'false',
                'discrepancy_type': 'error',
                'confidence': 'none',
                'notes': 'Exception during classification',
            })
            continue

        classified_tier = result.tier
        classified_category = result.subdirectory or ''
        classified_decade = result.decade or ''

        discrepancy = _discrepancy_type(result, current_tier, current_category)
        match = 'true' if discrepancy == '' else 'false'
        confidence = _confidence_label(result.reason, result.tmdb_id)
        notes = _build_notes(result, discrepancy)

        report_rows.append({
            'filename': filename,
            'current_tier': current_tier,
            'current_category': current_category,
            'current_decade': current_decade,
            'classified_tier': classified_tier,
            'classified_category': classified_category,
            'classified_decade': classified_decade,
            'match': match,
            'discrepancy_type': discrepancy,
            'confidence': confidence,
            'notes': notes,
        })

        if i % 100 == 0:
            logger.info("Processed %d/%d...", i, len(audit_rows))

    return report_rows


# ---------------------------------------------------------------------------
# Stage 2: --enrich
# ---------------------------------------------------------------------------

def enrich_no_data(report_rows: List[Dict], config_path: Path) -> int:
    """
    Re-classify no_data rows using live API queries. Updates rows in-place.
    Returns count of rows that resolved to a non-Unsorted tier.
    """
    no_data_rows = [r for r in report_rows if r['discrepancy_type'] == 'no_data']
    if not no_data_rows:
        logger.info("No no_data rows to enrich.")
        return 0

    logger.info("Enriching %d no_data films via live API...", len(no_data_rows))
    classifier = _make_classifier(config_path, live_api=True)
    parser = FilenameParser()
    resolved = 0

    for row in no_data_rows:
        filename = row['filename']
        try:
            metadata = parser.parse(filename)
            result = classifier.classify(metadata)
        except Exception as e:
            logger.error("Error enriching '%s': %s", filename, e)
            continue

        current_tier = row['current_tier']
        current_category = row['current_category']
        discrepancy = _discrepancy_type(result, current_tier, current_category)

        row['classified_tier'] = result.tier
        row['classified_category'] = result.subdirectory or ''
        row['classified_decade'] = result.decade or ''
        row['match'] = 'true' if discrepancy == '' else 'false'
        row['discrepancy_type'] = discrepancy
        row['confidence'] = _confidence_label(result.reason, result.tmdb_id)
        row['notes'] = _build_notes(result, discrepancy)

        if result.tier != 'Unsorted':
            resolved += 1

    logger.info("Enrichment: %d/%d no_data rows resolved to a tier", resolved, len(no_data_rows))
    return resolved


# ---------------------------------------------------------------------------
# Stage 3: --review
# ---------------------------------------------------------------------------

_ACTION_MAP = {
    'wrong_tier': '[RECLASSIFY]',
    'wrong_category': '[RECLASSIFY]',
    'no_data': '[MANUAL REVIEW]',
    'unroutable': '[INVESTIGATE ROUTING]',
    '': '[CONFIRMED]',
}

_CONFIDENCE_ORDER = {'high': 0, 'medium': 1, 'low': 2, 'none': 3}


def write_review_report(report_rows: List[Dict], output_path: Path):
    """Write grouped markdown review report."""
    # Group by current_category (or current_tier for tiers without categories)
    by_category: Dict[str, List[Dict]] = defaultdict(list)
    for row in report_rows:
        key = row['current_category'] or row['current_tier']
        by_category[key].append(row)

    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("# Library Re-Audit Review\n\n")
        f.write("Generated by `scripts/reaudit.py --review`\n\n")

        total = len(report_rows)
        matches = sum(1 for r in report_rows if r['match'] == 'true')
        f.write(f"**{total} films audited | {matches} confirmed | {total - matches} discrepancies**\n\n")
        f.write("---\n\n")

        for category in sorted(by_category.keys()):
            rows = sorted(by_category[category],
                          key=lambda r: (_CONFIDENCE_ORDER.get(r['confidence'], 3),
                                         r['filename']))
            discrepancies = [r for r in rows if r['match'] == 'false']
            confirmed = len(rows) - len(discrepancies)

            f.write(f"## {category}\n\n")
            f.write(f"{len(rows)} films — {confirmed} confirmed, {len(discrepancies)} discrepancies\n\n")

            if discrepancies:
                f.write("| Filename | Current | Classified | Type | Confidence | Action |\n")
                f.write("|---|---|---|---|---|---|\n")
                for row in rows:
                    if row['match'] == 'false':
                        current = f"{row['current_tier']}/{row['current_category']}/{row['current_decade']}".rstrip('/')
                        classified = f"{row['classified_tier']}/{row['classified_category']}/{row['classified_decade']}".rstrip('/')
                        action = _ACTION_MAP.get(row['discrepancy_type'], '')
                        f.write(f"| {row['filename']} | {current} | {classified} | "
                                f"{row['discrepancy_type']} | {row['confidence']} | {action} |\n")
                f.write("\n")
            else:
                f.write("All films confirmed.\n\n")

    logger.info("Wrote review report to %s", output_path)


# ---------------------------------------------------------------------------
# Output
# ---------------------------------------------------------------------------

def write_report_csv(report_rows: List[Dict], output_path: Path):
    """Write reaudit_report.csv."""
    output_path.parent.mkdir(parents=True, exist_ok=True)
    fieldnames = [
        'filename', 'current_tier', 'current_category', 'current_decade',
        'classified_tier', 'classified_category', 'classified_decade',
        'match', 'discrepancy_type', 'confidence', 'notes',
    ]
    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)
        writer.writeheader()
        writer.writerows(report_rows)
    logger.info("Wrote reaudit report to %s", output_path)


def print_summary(report_rows: List[Dict]):
    """Print summary statistics to stdout."""
    total = len(report_rows)
    matches = sum(1 for r in report_rows if r['match'] == 'true')
    by_type: Dict[str, int] = defaultdict(int)
    for r in report_rows:
        by_type[r['discrepancy_type'] or 'match'] += 1

    print(f"\n{'='*60}")
    print(f"REAUDIT SUMMARY")
    print(f"{'='*60}")
    print(f"Total organized films audited : {total}")
    print(f"Confirmed (match)             : {matches}")
    print(f"Discrepancies                 : {total - matches}")
    print()
    for dtype in ['wrong_tier', 'wrong_category', 'no_data', 'unroutable', 'error']:
        count = by_type.get(dtype, 0)
        if count:
            print(f"  {dtype:<20} : {count}")
    print(f"{'='*60}\n")


# ---------------------------------------------------------------------------
# Corpus check (--corpus flag, Issue #38)
# ---------------------------------------------------------------------------

def run_corpus_check(audit_rows: List[Dict], corpus_lookup: CorpusLookup) -> List[Dict]:
    """
    Cross-reference each organized film against the ground truth corpus.

    For each film:
      corpus_confirmed  — in corpus AND in the correct Satellite category folder
      corpus_mismatch   — in corpus BUT in a different category folder
      corpus_unconfirmed — not in corpus (no external verdict available)

    Returns list of dicts with: filename, current_category, corpus_category,
    corpus_tier, corpus_verdict, corpus_source.
    """
    parser = FilenameParser()
    rows = []

    for row in audit_rows:
        filename = row.get('filename', '')
        current_tier = row.get('tier', row.get('current_tier', ''))
        current_category = row.get('subdirectory', row.get('current_category', ''))

        meta = parser.parse(filename)
        corpus_hit = corpus_lookup.lookup(
            meta.title, meta.year, imdb_id=getattr(meta, 'imdb_id', None)
        )

        if corpus_hit is None:
            rows.append({
                'filename': filename,
                'current_tier': current_tier,
                'current_category': current_category,
                'corpus_category': '',
                'corpus_tier': '',
                'corpus_verdict': 'corpus_unconfirmed',
                'corpus_source': '',
            })
            continue

        corpus_category = corpus_hit['category']
        corpus_tier = corpus_hit['canonical_tier']
        corpus_source = corpus_hit.get('source', '')

        # Check whether the film is in the right place
        # Corpus categories are Satellite subcategories; compare against current_category
        if current_tier == 'Satellite' and current_category == corpus_category:
            verdict = 'corpus_confirmed'
        else:
            verdict = 'corpus_mismatch'

        rows.append({
            'filename': filename,
            'current_tier': current_tier,
            'current_category': current_category,
            'corpus_category': corpus_category,
            'corpus_tier': corpus_tier,
            'corpus_verdict': verdict,
            'corpus_source': corpus_source,
        })

    return rows


def print_corpus_summary(corpus_rows: List[Dict]) -> None:
    """Print corpus check summary to stdout."""
    by_verdict: Dict[str, List] = defaultdict(list)
    for r in corpus_rows:
        by_verdict[r['corpus_verdict']].append(r)

    total = len(corpus_rows)
    confirmed = len(by_verdict.get('corpus_confirmed', []))
    mismatch = len(by_verdict.get('corpus_mismatch', []))
    unconfirmed = len(by_verdict.get('corpus_unconfirmed', []))

    print(f"\n{'='*60}")
    print(f"CORPUS CHECK (external standard, Issue #38)")
    print(f"{'='*60}")
    print(f"Total films in audit:          {total}")
    print(f"In corpus + correct folder:    {confirmed}")
    print(f"In corpus + WRONG folder:      {mismatch}  ← real misclassifications")
    print(f"Not in corpus (no verdict):    {unconfirmed}")

    if mismatch:
        print(f"\nCORPUS MISMATCHES (films in wrong category):")
        for r in by_verdict['corpus_mismatch']:
            print(f"  {r['filename']}")
            print(f"    Current:  {r['current_tier']}/{r['current_category']}")
            print(f"    Corpus:   Satellite/{r['corpus_category']} (tier {r['corpus_tier']}, source: {r['corpus_source']})")

    print(f"{'='*60}\n")


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(
        description='Re-audit organized library against current classification rules (Issue #31)'
    )
    parser.add_argument(
        '--audit',
        default='output/library_audit.csv',
        help='Path to library_audit.csv (default: output/library_audit.csv)'
    )
    parser.add_argument(
        '--enrich',
        action='store_true',
        help='Stage 2: query live APIs for no_data films (costs API calls)'
    )
    parser.add_argument(
        '--review',
        action='store_true',
        help='Stage 3: write human-readable markdown report to output/reaudit_review.md'
    )
    parser.add_argument(
        '--config',
        default='config.yaml',
        help='Path to config YAML (default: config.yaml)'
    )
    parser.add_argument(
        '--corpus',
        action='store_true',
        help='Run corpus check against data/corpora/ CSVs (external standard, Issue #38)'
    )
    args = parser.parse_args()

    config_path = Path(args.config)
    if not config_path.exists():
        # Try config_external.yaml as fallback
        config_path = Path('config_external.yaml')
    if not config_path.exists():
        logger.error("No config file found (tried config.yaml, config_external.yaml)")
        sys.exit(1)

    audit_path = Path(args.audit)

    # Stage 1: cache-only classification pass
    logger.info("Stage 1: cache-only classification pass")
    classifier = _make_classifier(config_path, live_api=False)
    audit_rows = _load_audit_csv(audit_path)
    report_rows = run_audit_pass(audit_rows, classifier)

    # Stage 2: enrich no_data films (optional)
    if args.enrich:
        logger.info("Stage 2: enriching no_data films via live API")
        enrich_no_data(report_rows, config_path)

    # Write CSV report
    write_report_csv(report_rows, Path('output/reaudit_report.csv'))
    print_summary(report_rows)

    # Stage 3: markdown review report (optional)
    if args.review:
        logger.info("Stage 3: writing review report")
        write_review_report(report_rows, Path('output/reaudit_review.md'))

    # Stage 4: corpus check (optional, Issue #38)
    if args.corpus:
        corpora_dir = Path(__file__).parent.parent / 'data' / 'corpora'
        corpus_lookup = CorpusLookup(corpora_dir)
        stats = corpus_lookup.get_stats()
        if stats['total_entries'] == 0:
            logger.warning("No corpus files found in %s — skipping corpus check", corpora_dir)
        else:
            logger.info("Stage 4: corpus check (%d films across %d categories)",
                        stats['total_entries'], len(stats['categories']))
            corpus_rows = run_corpus_check(audit_rows, corpus_lookup)
            print_corpus_summary(corpus_rows)
            # Write corpus report CSV
            corpus_report_path = Path('output/corpus_check_report.csv')
            corpus_report_path.parent.mkdir(parents=True, exist_ok=True)
            with open(corpus_report_path, 'w', newline='', encoding='utf-8') as f:
                fieldnames = ['filename', 'current_tier', 'current_category',
                              'corpus_category', 'corpus_tier', 'corpus_verdict', 'corpus_source']
                writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)
                writer.writeheader()
                writer.writerows(corpus_rows)
            logger.info("Wrote corpus check report to %s", corpus_report_path)


if __name__ == '__main__':
    main()
