#!/usr/bin/env python3
"""
Failure Cohort Analysis â€” Issue #35 Stage 4+5

Reads output/evidence_trails.csv (generated by classify.py) and groups unsorted
films by their evidence pattern, producing named failure cohorts with hypotheses.

Usage:
    python scripts/analyze_cohorts.py
    python scripts/analyze_cohorts.py --csv output/evidence_trails.csv
    python scripts/analyze_cohorts.py --min-cohort-size 3
"""

import argparse
import csv
import json
from collections import defaultdict
from dataclasses import dataclass, field
from datetime import date
from pathlib import Path
from typing import Dict, List, Optional, Tuple


# ---------------------------------------------------------------------------
# Cohort data structure
# ---------------------------------------------------------------------------

@dataclass
class Cohort:
    cohort_type: str           # cap_exceeded | data_gap | director_gap | gate_design_gap | taxonomy_gap
    name: str                  # human label
    key: str                   # grouping key (used for dedup)
    films: List[dict] = field(default_factory=list)
    binding_constraint: str = ''
    hypothesis: str = ''
    confidence: str = 'LOW'    # HIGH | MEDIUM | LOW
    category: str = ''         # affected category if applicable
    director: str = ''         # affected director if applicable
    country: str = ''          # affected country if applicable

    @property
    def count(self):
        return len(self.films)


# ---------------------------------------------------------------------------
# Data loading
# ---------------------------------------------------------------------------

def load_evidence_trails(csv_path: Path) -> List[dict]:
    with open(csv_path, newline='', encoding='utf-8') as f:
        return list(csv.DictReader(f))


def is_unsorted(row: dict) -> bool:
    return 'Unsorted' in row.get('tier', '') or row.get('reason', '').startswith('unsorted_')


def parse_gates_missing(row: dict) -> List[str]:
    """Return list of gate names from gates_missing_for_nearest."""
    raw = row.get('gates_missing_for_nearest', '')
    return [g.strip() for g in raw.split(',') if g.strip()]


def parse_fields_absent(row: dict) -> List[str]:
    raw = row.get('fields_absent', '')
    return [f.strip() for f in raw.split(',') if f.strip()]


def decade_from_year(year_str: str) -> str:
    try:
        y = int(year_str)
        return f"{str(y)[:3]}0s"
    except (ValueError, TypeError):
        return '?'


# ---------------------------------------------------------------------------
# Cohort type detection
# ---------------------------------------------------------------------------

def classify_cohort_type(film: dict) -> Optional[str]:
    """Return the primary cohort type for an unsorted film, or None."""
    nm = film.get('nearest_miss', '')
    gates = parse_gates_missing(film)
    fields_absent = parse_fields_absent(film)
    dr = film.get('data_readiness', '')

    # Cap exceeded: all gates pass but film didn't route
    if nm and not gates:
        return 'cap_exceeded'

    # Data gap: missing critical API fields AND has a nearest miss
    if fields_absent and nm:
        return 'data_gap'

    # Director gap: R3 data (nothing absent), but director gate is the only blocker
    if dr == 'R3' and not fields_absent and nm and gates == ['director_gate']:
        return 'director_gap'

    # Director gap (with minor secondary gates like title_kw_gate)
    if dr == 'R3' and not fields_absent and nm and 'director_gate' in gates and len(gates) <= 2:
        # Only flag as director_gap if director_gate is the primary blocker
        non_director_gates = [g for g in gates if g != 'director_gate' and g != 'title_kw_gate']
        if not non_director_gates:
            return 'director_gap'

    # Gate design gap: near-miss on 1 gate with full data
    if nm and len(gates) == 1 and not fields_absent:
        return 'gate_design_gap'

    # Taxonomy gap: has routing data but NO nearest miss category
    if not nm and dr in ('R2', 'R3') and (film.get('director', '') or film.get('country', '')):
        return 'taxonomy_gap'

    return None  # R0/R1 or unclassifiable


# ---------------------------------------------------------------------------
# Cohort analyzers
# ---------------------------------------------------------------------------

def analyze_cap_exceeded(unsorted: List[dict]) -> List[Cohort]:
    """Films that pass all gates for their nearest_miss but didn't route (cap hit)."""
    candidates = [
        f for f in unsorted
        if classify_cohort_type(f) == 'cap_exceeded'
    ]

    groups = defaultdict(list)
    for film in candidates:
        groups[film['nearest_miss']].append(film)

    cohorts = []
    for category, films in sorted(groups.items(), key=lambda x: -len(x[1])):
        n = len(films)
        confidence = 'HIGH' if n >= 3 else 'MEDIUM' if n == 2 else 'LOW'
        cohorts.append(Cohort(
            cohort_type='cap_exceeded',
            name=f'Cap exceeded â€” {category} ({n} films)',
            key=f'cap/{category}',
            films=films,
            binding_constraint=f'{category} category cap was exceeded during this classify run',
            hypothesis=(
                f'These {n} films pass ALL gates for {category} but were blocked by the '
                f'category cap. Re-run classify.py â€” if the cap has been raised, they will '
                f'route automatically. If not, raise the cap in lib/satellite.py.'
            ),
            confidence=confidence,
            category=category,
        ))
    return cohorts


def analyze_data_gaps(unsorted: List[dict]) -> List[Cohort]:
    """Films with missing API fields that block routing."""
    candidates = [
        f for f in unsorted
        if classify_cohort_type(f) == 'data_gap'
    ]

    # Group by (fields_absent set, nearest_miss_category)
    groups = defaultdict(list)
    for film in candidates:
        absent = ','.join(sorted(parse_fields_absent(film)))
        nm = film.get('nearest_miss', '')
        groups[(absent, nm)].append(film)

    cohorts = []
    for (absent, nm), films in sorted(groups.items(), key=lambda x: -len(x[1])):
        n = len(films)
        confidence = 'MEDIUM' if n >= 10 else 'LOW' if n < 5 else 'MEDIUM'

        absent_display = absent.replace(',', ' + ')
        nm_display = f' â†’ nearest: {nm}' if nm else ''
        name = f'Missing {absent_display}{nm_display} ({n} films)'

        cohorts.append(Cohort(
            cohort_type='data_gap',
            name=name,
            key=f'data_gap/{absent}/{nm}',
            films=films,
            binding_constraint=f'API fields absent: {absent}',
            hypothesis=(
                f'These {n} films are blocked by missing {absent_display} data. '
                f'{"Nearest match is " + nm + " â€” enriching " + absent_display + " would likely resolve routing. " if nm else ""}'
                f'Add known data to output/manual_enrichment.csv and re-run classify.'
            ),
            confidence=confidence,
            category=nm,
        ))
    return cohorts


def analyze_director_gaps(unsorted: List[dict]) -> List[Cohort]:
    """R3 films blocked because director is not in a category's directors list."""
    candidates = [
        f for f in unsorted
        if classify_cohort_type(f) == 'director_gap'
    ]

    # Group by (normalized director surname, nearest_miss_category)
    groups = defaultdict(list)
    for film in candidates:
        raw_dir = film.get('director', '').strip()
        # Use last word as surname key for grouping (e.g. "Dennis Hopper" â†’ "hopper")
        surname = raw_dir.split()[-1].lower() if raw_dir else '?'
        nm = film.get('nearest_miss', '')
        groups[(surname, nm)].append(film)

    cohorts = []
    for (surname, nm), films in sorted(groups.items(), key=lambda x: -len(x[1])):
        n = len(films)
        # Representative director name (most common full name in the group)
        from collections import Counter
        dir_counts = Counter(f['director'] for f in films)
        rep_director = dir_counts.most_common(1)[0][0]

        confidence = 'HIGH' if n >= 3 else 'MEDIUM' if n == 2 else 'LOW'

        cohorts.append(Cohort(
            cohort_type='director_gap',
            name=f'{rep_director} â€” {nm} ({n} films)',
            key=f'director/{surname}/{nm}',
            films=films,
            binding_constraint=f'Director "{rep_director}" not in {nm} directors list',
            hypothesis=(
                f'Add "{surname}" (or the full name "{rep_director}") to '
                f'SATELLITE_ROUTING_RULES[\'{nm}\'][\'directors\'] in lib/constants.py. '
                f'This would immediately route {n} film{"s" if n > 1 else ""} to {nm}. '
                f'Verify director membership in the {nm} movement before adding.'
            ),
            confidence=confidence,
            category=nm,
            director=rep_director,
        ))
    return cohorts


def analyze_gate_design_gaps(unsorted: List[dict]) -> List[Cohort]:
    """Films 1 gate away from routing (with complete data)."""
    candidates = [
        f for f in unsorted
        if classify_cohort_type(f) == 'gate_design_gap'
    ]

    # Group by (nearest_miss_category, blocking_gate)
    groups = defaultdict(list)
    for film in candidates:
        gates = parse_gates_missing(film)
        nm = film.get('nearest_miss', '')
        gate = gates[0] if gates else '?'
        groups[(nm, gate)].append(film)

    cohorts = []
    for (nm, gate), films in sorted(groups.items(), key=lambda x: -len(x[1])):
        n = len(films)
        confidence = 'MEDIUM' if n >= 5 else 'LOW' if n < 3 else 'LOW'

        # Special case: country_gate blocking â€” often means these films don't belong to the category
        if gate == 'country_gate':
            hypothesis = (
                f'{n} films are 1 gate (country_gate) away from {nm}. '
                f'The country gate is correctly blocking films from countries outside {nm}\'s scope. '
                f'These films likely need a different category or SORTING_DATABASE pins. '
                f'Do NOT relax the country gate for {nm}.'
            )
        elif gate == 'genre_gate':
            hypothesis = (
                f'{n} films pass country + decade for {nm} but fail the genre gate. '
                f'Options: (1) enrich genre data for these films via manual_enrichment.csv, '
                f'(2) check if genres are simply absent from API (fields_absent) vs genuinely wrong genre, '
                f'(3) for negative-space categories (Indie Cinema), consider making genre gate advisory for R2 films.'
            )
        elif gate == 'director_gate':
            hypothesis = (
                f'{n} films pass all gates except director_gate for {nm}. '
                f'Audit whether the directors of these films belong to {nm}. '
                f'If yes, add to SATELLITE_ROUTING_RULES[\'{nm}\'][\'directors\'].'
            )
        else:
            hypothesis = (
                f'{n} films are 1 gate ({gate}) away from {nm}. '
                f'Review whether relaxing {gate} for {nm} is historically justified. '
                f'See docs/SATELLITE_CATEGORIES.md for {nm} specification.'
            )

        cohorts.append(Cohort(
            cohort_type='gate_design_gap',
            name=f'{nm} â€” {gate} blocking ({n} films)',
            key=f'gate_design/{nm}/{gate}',
            films=films,
            binding_constraint=f'{gate} fails for {nm}',
            hypothesis=hypothesis,
            confidence=confidence,
            category=nm,
        ))
    return cohorts


def analyze_taxonomy_gaps(unsorted: List[dict]) -> List[Cohort]:
    """Films with routing data but no category covers their country/era."""
    candidates = [
        f for f in unsorted
        if classify_cohort_type(f) == 'taxonomy_gap'
        and f.get('country', '')  # needs a country to be meaningful
    ]

    # Group by (country, decade)
    groups = defaultdict(list)
    for film in candidates:
        decade = decade_from_year(film.get('year', ''))
        key = (film.get('country', '?'), decade)
        groups[key].append(film)

    cohorts = []
    for (country, decade), films in sorted(groups.items(), key=lambda x: -len(x[1])):
        n = len(films)
        confidence = 'MEDIUM' if n >= 5 else 'LOW' if n < 3 else 'LOW'

        cohorts.append(Cohort(
            cohort_type='taxonomy_gap',
            name=f'Country gap: {country} / {decade} ({n} films)',
            key=f'taxonomy/{country}/{decade}',
            films=films,
            binding_constraint=f'No Satellite category covers country={country} in {decade}',
            hypothesis=(
                f'{n} films from {country} ({decade}) have routing data but no category matches. '
                f'Before adding a new category: (1) verify density threshold (â‰¥10 films minimum), '
                f'(2) confirm historical movement exists (Domain Grounding â€” see CLAUDE.md Rule 4), '
                f'(3) check if country should be added to Indie Cinema country_codes in lib/constants.py.'
            ),
            confidence=confidence,
            country=country,
        ))
    return cohorts


# ---------------------------------------------------------------------------
# R1 summary (not a cohort, just a count)
# ---------------------------------------------------------------------------

def summarize_r1(unsorted: List[dict]) -> dict:
    r1_films = [f for f in unsorted if f.get('data_readiness') == 'R1']
    r0_films = [f for f in unsorted if f.get('data_readiness') == 'R0' or f.get('reason') == 'unsorted_no_year']
    return {
        'r0_count': len(r0_films),
        'r1_count': len(r1_films),
        'r0_note': 'Films with no year â€” supplements, interviews, non-film content. Not analyzed further.',
        'r1_note': 'Films with year but no API data (no director, no country). '
                   'Remedy: run with OMDb/TMDb API keys, or add to output/manual_enrichment.csv.',
    }


# ---------------------------------------------------------------------------
# Confidence helper
# ---------------------------------------------------------------------------

def assign_confidence(cohort: Cohort) -> str:
    n = cohort.count
    t = cohort.cohort_type
    if t == 'cap_exceeded':
        return 'HIGH' if n >= 3 else 'MEDIUM'
    if t == 'director_gap':
        return 'HIGH' if n >= 3 else 'MEDIUM' if n == 2 else 'LOW'
    if t == 'data_gap':
        return 'MEDIUM' if n >= 5 else 'LOW'
    if t == 'gate_design_gap':
        return 'MEDIUM' if n >= 5 else 'LOW'
    if t == 'taxonomy_gap':
        return 'MEDIUM' if n >= 5 else 'LOW'
    return 'LOW'


# ---------------------------------------------------------------------------
# Output: cohorts_report.md
# ---------------------------------------------------------------------------

CONFIDENCE_EMOJI = {'HIGH': 'ðŸŸ¢', 'MEDIUM': 'ðŸŸ¡', 'LOW': 'ðŸ”´'}

ACTION_LABELS = {
    'cap_exceeded': 'Re-run classify',
    'data_gap': 'Enrich data',
    'director_gap': 'Add director',
    'gate_design_gap': 'Review gate',
    'taxonomy_gap': 'Review taxonomy',
}


def _film_table(films: List[dict], max_rows: int = 20) -> str:
    lines = ['| Title | Year | Director | Country | Nearest miss |',
             '|-------|------|----------|---------|--------------|']
    for f in films[:max_rows]:
        title = f.get('title', '')[:30]
        year = f.get('year', '')
        director = f.get('director', '')[:22]
        country = f.get('country', '')
        nm = f.get('nearest_miss', '')[:20]
        lines.append(f'| {title} | {year} | {director} | {country} | {nm} |')
    if len(films) > max_rows:
        lines.append(f'| *...and {len(films) - max_rows} more* | | | | |')
    return '\n'.join(lines)


def write_report(cohorts: List[Cohort], r1_summary: dict, output_path: Path,
                 total_unsorted: int, total_films: int) -> None:
    today = date.today().isoformat()
    actionable = [c for c in cohorts if c.confidence in ('HIGH', 'MEDIUM')]

    lines = [
        f'# Failure Cohort Analysis',
        f'',
        f'Generated: {today} | Films analyzed: {total_films} total, {total_unsorted} unsorted | Actionable cohorts: {len(actionable)}',
        f'',
        f'---',
        f'',
        f'## Data Readiness Summary',
        f'',
        f'| Level | Count | Notes |',
        f'|-------|-------|-------|',
        f'| R0 (no year) | {r1_summary["r0_count"]} | {r1_summary["r0_note"][:60]}... |',
        f'| R1 (no API data) | {r1_summary["r1_count"]} | {r1_summary["r1_note"][:60]}... |',
        f'| R2/R3 (actionable) | {total_unsorted - r1_summary["r0_count"] - r1_summary["r1_count"]} | Analyzed below |',
        f'',
        f'---',
        f'',
        f'## Cohort Summary',
        f'',
        f'| # | Cohort | Type | Films | Confidence | Action |',
        f'|---|--------|------|-------|------------|--------|',
    ]

    for i, cohort in enumerate(cohorts, 1):
        conf = CONFIDENCE_EMOJI.get(cohort.confidence, '') + ' ' + cohort.confidence
        action = ACTION_LABELS.get(cohort.cohort_type, 'Review')
        lines.append(f'| {i} | {cohort.name} | {cohort.cohort_type} | {cohort.count} | {conf} | {action} |')

    lines += ['', '---', '']

    for i, cohort in enumerate(cohorts, 1):
        conf_label = f'{CONFIDENCE_EMOJI.get(cohort.confidence, "")} **{cohort.confidence}**'
        lines += [
            f'## [{i}] {cohort.name}',
            f'',
            f'**Type:** `{cohort.cohort_type}` | **Confidence:** {conf_label}',
            f'',
            f'**Binding constraint:** {cohort.binding_constraint}',
            f'',
            f'**Hypothesis:**',
            f'> {cohort.hypothesis}',
            f'',
        ]

        if cohort.films:
            lines.append(_film_table(cohort.films))
            lines.append('')

        lines.append('---')
        lines.append('')

    # R1 summary section at the end
    lines += [
        f'## No-API-Data Films (R1 â€” {r1_summary["r1_count"]} films)',
        f'',
        f'These films have a year but returned no director or country from TMDb/OMDb.',
        f'They are not analyzed as individual cohorts because the root cause is data absence,',
        f'not a routing rule gap.',
        f'',
        f'**Remedy:** Add known director/country to `output/manual_enrichment.csv` for',
        f'individual films, or investigate title parsing if API lookups are failing.',
        f'',
    ]

    output_path.write_text('\n'.join(lines), encoding='utf-8')
    print(f'Wrote {output_path}')


# ---------------------------------------------------------------------------
# Output: failure_cohorts.json
# ---------------------------------------------------------------------------

def write_json(cohorts: List[Cohort], output_path: Path) -> None:
    data = []
    for c in cohorts:
        data.append({
            'cohort_type': c.cohort_type,
            'name': c.name,
            'key': c.key,
            'count': c.count,
            'confidence': c.confidence,
            'binding_constraint': c.binding_constraint,
            'hypothesis': c.hypothesis,
            'category': c.category,
            'director': c.director,
            'country': c.country,
            'films': [
                {
                    'filename': f.get('filename', ''),
                    'title': f.get('title', ''),
                    'year': f.get('year', ''),
                    'director': f.get('director', ''),
                    'country': f.get('country', ''),
                    'nearest_miss': f.get('nearest_miss', ''),
                    'gates_missing_for_nearest': f.get('gates_missing_for_nearest', ''),
                    'fields_absent': f.get('fields_absent', ''),
                }
                for f in c.films
            ],
        })
    output_path.write_text(json.dumps(data, indent=2, ensure_ascii=False), encoding='utf-8')
    print(f'Wrote {output_path}')


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(description='Analyze failure cohorts from evidence_trails.csv')
    parser.add_argument('--csv', default='output/evidence_trails.csv',
                        help='Path to evidence_trails.csv (default: output/evidence_trails.csv)')
    parser.add_argument('--output-dir', default='output',
                        help='Output directory for reports (default: output/)')
    parser.add_argument('--min-cohort-size', type=int, default=2,
                        help='Minimum cohort size to include in report (default: 2)')
    args = parser.parse_args()

    csv_path = Path(args.csv)
    output_dir = Path(args.output_dir)

    if not csv_path.exists():
        print(f'ERROR: {csv_path} not found. Run classify.py first.')
        return 1

    print(f'Loading {csv_path}...')
    rows = load_evidence_trails(csv_path)
    unsorted = [r for r in rows if is_unsorted(r)]

    print(f'Total films: {len(rows)}, Unsorted: {len(unsorted)}')

    # Run all analyzers
    all_cohorts: List[Cohort] = []
    all_cohorts += analyze_cap_exceeded(unsorted)
    all_cohorts += analyze_director_gaps(unsorted)
    all_cohorts += analyze_gate_design_gaps(unsorted)
    all_cohorts += analyze_data_gaps(unsorted)
    all_cohorts += analyze_taxonomy_gaps(unsorted)

    # Assign confidence and filter by min size
    for c in all_cohorts:
        c.confidence = assign_confidence(c)

    cohorts = [c for c in all_cohorts if c.count >= args.min_cohort_size]

    # Sort: HIGH first, then MEDIUM, then LOW; within tier by count desc
    order = {'HIGH': 0, 'MEDIUM': 1, 'LOW': 2}
    cohorts.sort(key=lambda c: (order.get(c.confidence, 3), -c.count))

    r1_summary = summarize_r1(unsorted)

    print(f'Cohorts found: {len(cohorts)} (min size {args.min_cohort_size})')

    # Write outputs
    report_path = output_dir / 'cohorts_report.md'
    json_path = output_dir / 'failure_cohorts.json'

    write_report(cohorts, r1_summary, report_path, len(unsorted), len(rows))
    write_json(cohorts, json_path)

    # Print summary to stdout
    print()
    print('=' * 60)
    print('COHORT ANALYSIS SUMMARY')
    print('=' * 60)
    high = [c for c in cohorts if c.confidence == 'HIGH']
    medium = [c for c in cohorts if c.confidence == 'MEDIUM']
    low = [c for c in cohorts if c.confidence == 'LOW']
    print(f'  HIGH   confidence: {len(high)} cohorts ({sum(c.count for c in high)} films)')
    print(f'  MEDIUM confidence: {len(medium)} cohorts ({sum(c.count for c in medium)} films)')
    print(f'  LOW    confidence: {len(low)} cohorts ({sum(c.count for c in low)} films)')
    print()
    if high:
        print('Immediate actions:')
        for c in high:
            print(f'  [{c.cohort_type:15s}] {c.name}')
    print()
    print(f'Full report: {report_path}')
    return 0


if __name__ == '__main__':
    raise SystemExit(main())
